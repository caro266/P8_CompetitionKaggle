{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/29762/logos/header.png)\n\n# Context\n\nThe aim of this competition is to classify more than 81K landmark classes.\n\nFor each test image, we have to predict one landmark label and a corresponding confidence score. ","metadata":{}},{"cell_type":"markdown","source":"# Libraries\n\nWe use some standard python packages and the libraries of scikit learn and keras. ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\n\nfrom keras.utils import to_categorical, Sequence\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50, ResNet152V2, NASNetMobile\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers.normalization import BatchNormalization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:34.099341Z","iopub.execute_input":"2021-09-30T23:10:34.099688Z","iopub.status.idle":"2021-09-30T23:10:34.106988Z","shell.execute_reply.started":"2021-09-30T23:10:34.099657Z","shell.execute_reply":"2021-09-30T23:10:34.105865Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Path","metadata":{}},{"cell_type":"code","source":"#path = '../../landmark-recognition-2021/'\npath = '/kaggle/input/landmark-recognition-2021/'\nos.listdir(path)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:34.109151Z","iopub.execute_input":"2021-09-30T23:10:34.109786Z","iopub.status.idle":"2021-09-30T23:10:34.124957Z","shell.execute_reply.started":"2021-09-30T23:10:34.109745Z","shell.execute_reply":"2021-09-30T23:10:34.124111Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\n## Load","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(path+'train.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:34.127587Z","iopub.execute_input":"2021-09-30T23:10:34.127867Z","iopub.status.idle":"2021-09-30T23:10:35.227351Z","shell.execute_reply.started":"2021-09-30T23:10:34.127842Z","shell.execute_reply":"2021-09-30T23:10:35.226479Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.229009Z","iopub.execute_input":"2021-09-30T23:10:35.229376Z","iopub.status.idle":"2021-09-30T23:10:35.240760Z","shell.execute_reply.started":"2021-09-30T23:10:35.229346Z","shell.execute_reply":"2021-09-30T23:10:35.239561Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"samp_subm.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.242431Z","iopub.execute_input":"2021-09-30T23:10:35.242823Z","iopub.status.idle":"2021-09-30T23:10:35.255800Z","shell.execute_reply.started":"2021-09-30T23:10:35.242787Z","shell.execute_reply":"2021-09-30T23:10:35.255038Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"samp_subm =samp_subm[:100]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.256879Z","iopub.execute_input":"2021-09-30T23:10:35.257231Z","iopub.status.idle":"2021-09-30T23:10:35.261608Z","shell.execute_reply.started":"2021-09-30T23:10:35.257197Z","shell.execute_reply":"2021-09-30T23:10:35.260532Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Analyse","metadata":{}},{"cell_type":"code","source":"print(\"Mean picture per class : \",np.mean(train_data[\"landmark_id\"].value_counts()))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.262999Z","iopub.execute_input":"2021-09-30T23:10:35.263379Z","iopub.status.idle":"2021-09-30T23:10:35.306267Z","shell.execute_reply.started":"2021-09-30T23:10:35.263343Z","shell.execute_reply":"2021-09-30T23:10:35.305271Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30, 15))\nplt.boxplot(train_data[\"landmark_id\"].value_counts(), vert=0)\nplt.title(\"BoxPlot du nombre d'image par catégorie\",fontsize=30)\nplt.xticks(size=20)\nplt.xlabel(\"Nbr Image\",fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.307602Z","iopub.execute_input":"2021-09-30T23:10:35.307957Z","iopub.status.idle":"2021-09-30T23:10:35.549944Z","shell.execute_reply.started":"2021-09-30T23:10:35.307919Z","shell.execute_reply":"2021-09-30T23:10:35.549069Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# We keep 100 classes\nnClasses= 10\nstart = 100\nindexToUse = train_data[\"landmark_id\"].value_counts()[start:nClasses+start].index\nprint(indexToUse)\ntrain_data = train_data.query('landmark_id in @indexToUse')\ntrain_data=train_data.replace(indexToUse, range(nClasses))\n\n#train_data.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.553206Z","iopub.execute_input":"2021-09-30T23:10:35.553486Z","iopub.status.idle":"2021-09-30T23:10:35.637505Z","shell.execute_reply.started":"2021-09-30T23:10:35.553457Z","shell.execute_reply":"2021-09-30T23:10:35.635965Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(\"Mean picture per class (10 choosen classes): \",np.mean(train_data[\"landmark_id\"].value_counts()))\n\nfig = plt.figure(figsize=(30, 15))\nplt.boxplot(train_data[\"landmark_id\"].value_counts(), vert=0)\nplt.title(\"BoxPlot du nombre d'image par catégorie\",fontsize=30)\nplt.xticks(size=20)\nplt.xlabel(\"Nbr Image\",fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.639798Z","iopub.execute_input":"2021-09-30T23:10:35.640189Z","iopub.status.idle":"2021-09-30T23:10:35.820838Z","shell.execute_reply.started":"2021-09-30T23:10:35.640147Z","shell.execute_reply":"2021-09-30T23:10:35.819674Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def plot_examples(landmark_id=1):\n    \"\"\" Plot 5 examples of images with the same landmark_id \"\"\"\n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    for i in range(5):\n        idx = train_data[train_data['landmark_id']==landmark_id].index[i]\n        image_id = train_data.loc[idx, 'id']\n        file = image_id+'.jpg'\n        subpath = '/'.join([char for char in image_id[0:3]])\n        img = cv2.imread(path+'train/'+subpath+'/'+file)\n        axs[i].imshow(img)\n        axs[i].set_title('landmark_id: '+str(landmark_id))\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.822509Z","iopub.execute_input":"2021-09-30T23:10:35.822896Z","iopub.status.idle":"2021-09-30T23:10:35.831837Z","shell.execute_reply.started":"2021-09-30T23:10:35.822854Z","shell.execute_reply":"2021-09-30T23:10:35.830658Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def ImgToTreat(img):  \n    \"\"\" Image processing (histogram equalization and blur)\"\"\"\n    imgBlur = img\n    try:\n        locImg=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n        # Egalisation histogramme\n        imgEq = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n        imgEq[:, :, 0] = cv2.equalizeHist(imgEq[:, :, 0])\n        imgEq = cv2.cvtColor(imgEq,cv2.COLOR_YCrCb2RGB)\n        # Lissage bruit\n        imgBlur = cv2.blur(imgEq,(2,2))\n    except Exception as e:\n        print(e)\n        pass\n    return imgBlur","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.833678Z","iopub.execute_input":"2021-09-30T23:10:35.834102Z","iopub.status.idle":"2021-09-30T23:10:35.843191Z","shell.execute_reply.started":"2021-09-30T23:10:35.834066Z","shell.execute_reply":"2021-09-30T23:10:35.842288Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Overview\nFirst we look on the size of the dataset:","metadata":{}},{"cell_type":"code","source":"print('Samples train:', len(train_data))\nprint('Samples test:', len(samp_subm))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.846313Z","iopub.execute_input":"2021-09-30T23:10:35.846617Z","iopub.status.idle":"2021-09-30T23:10:35.854485Z","shell.execute_reply.started":"2021-09-30T23:10:35.846592Z","shell.execute_reply":"2021-09-30T23:10:35.853414Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"num_classes = len(train_data['landmark_id'].unique())\nprint(\"There is %d unique classes.\" %num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.856240Z","iopub.execute_input":"2021-09-30T23:10:35.856832Z","iopub.status.idle":"2021-09-30T23:10:35.865964Z","shell.execute_reply.started":"2021-09-30T23:10:35.856790Z","shell.execute_reply":"2021-09-30T23:10:35.864684Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.867957Z","iopub.execute_input":"2021-09-30T23:10:35.868609Z","iopub.status.idle":"2021-09-30T23:10:35.881152Z","shell.execute_reply.started":"2021-09-30T23:10:35.868570Z","shell.execute_reply":"2021-09-30T23:10:35.880277Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Image","metadata":{"execution":{"iopub.execute_input":"2021-08-14T07:51:34.634214Z","iopub.status.busy":"2021-08-14T07:51:34.633814Z"}}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.882488Z","iopub.execute_input":"2021-09-30T23:10:35.883215Z","iopub.status.idle":"2021-09-30T23:10:35.892994Z","shell.execute_reply.started":"2021-09-30T23:10:35.883175Z","shell.execute_reply":"2021-09-30T23:10:35.891973Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"image_id = train_data.loc[101440, 'id']\nfile = image_id+'.jpg'\nsubpath = '/'.join([char for char in image_id[0:3]]) ","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.894371Z","iopub.execute_input":"2021-09-30T23:10:35.894779Z","iopub.status.idle":"2021-09-30T23:10:35.901230Z","shell.execute_reply.started":"2021-09-30T23:10:35.894741Z","shell.execute_reply":"2021-09-30T23:10:35.900092Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"file","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.902690Z","iopub.execute_input":"2021-09-30T23:10:35.903213Z","iopub.status.idle":"2021-09-30T23:10:35.911503Z","shell.execute_reply.started":"2021-09-30T23:10:35.903177Z","shell.execute_reply":"2021-09-30T23:10:35.910348Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"subpath","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.913429Z","iopub.execute_input":"2021-09-30T23:10:35.914111Z","iopub.status.idle":"2021-09-30T23:10:35.920319Z","shell.execute_reply.started":"2021-09-30T23:10:35.914055Z","shell.execute_reply":"2021-09-30T23:10:35.919355Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Is the file located in the subpath?","metadata":{}},{"cell_type":"code","source":"file in os.listdir(path+'train/'+subpath)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.921901Z","iopub.execute_input":"2021-09-30T23:10:35.922662Z","iopub.status.idle":"2021-09-30T23:10:35.954077Z","shell.execute_reply.started":"2021-09-30T23:10:35.922616Z","shell.execute_reply":"2021-09-30T23:10:35.953055Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Plot the image:","metadata":{}},{"cell_type":"code","source":"img = cv2.imread(path+'train/'+subpath+'/'+file)\nplt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:35.955428Z","iopub.execute_input":"2021-09-30T23:10:35.955818Z","iopub.status.idle":"2021-09-30T23:10:36.179263Z","shell.execute_reply.started":"2021-09-30T23:10:35.955783Z","shell.execute_reply":"2021-09-30T23:10:36.178445Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Plot An Example\nWe plot an example of images with the same **landmark_id** in a row.","metadata":{}},{"cell_type":"code","source":"plot_examples(landmark_id = 1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-30T23:10:36.180418Z","iopub.execute_input":"2021-09-30T23:10:36.180935Z","iopub.status.idle":"2021-09-30T23:10:37.219416Z","shell.execute_reply.started":"2021-09-30T23:10:36.180891Z","shell.execute_reply":"2021-09-30T23:10:37.218435Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Image processing and data creation","metadata":{"execution":{"iopub.execute_input":"2021-09-07T11:03:28.222307Z","iopub.status.busy":"2021-09-07T11:03:28.221721Z","iopub.status.idle":"2021-09-07T11:03:28.247080Z","shell.execute_reply":"2021-09-07T11:03:28.246183Z","shell.execute_reply.started":"2021-09-07T11:03:28.222272Z"}}},{"cell_type":"code","source":"img_size = 224\nimg_channel = 3\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:37.220945Z","iopub.execute_input":"2021-09-30T23:10:37.221309Z","iopub.status.idle":"2021-09-30T23:10:37.225839Z","shell.execute_reply.started":"2021-09-30T23:10:37.221271Z","shell.execute_reply":"2021-09-30T23:10:37.224901Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"labels = []\nimage_path = []\nimages_pixels = []\nfor row in train_data.itertuples():\n    image_id = row.id\n    file = image_id+'.jpg'\n    subpath = '/'.join([char for char in image_id[0:3]]) \n    finalpath = path+\"/train/\"+subpath+'/'+file\n\n    img = cv2.imread(finalpath)\n    img = cv2.resize(img, (img_size, img_size))\n    img = ImgToTreat(img)\n\n    images_pixels.append(img)\n    #image_path.append(finalpath)\n    labels.append(row.landmark_id) ","metadata":{"execution":{"iopub.status.busy":"2021-09-30T23:10:37.227502Z","iopub.execute_input":"2021-09-30T23:10:37.228144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#labels=to_categorical(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data\nWe define train, validation and test data.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(images_pixels,\n                                                  labels, test_size = 0.3, \n                                                  random_state=101)\nprint(\"X train data : \", len(X_train))\nprint(\"X label data : \", len(X_val))\nprint(\"Y test data : \", len(Y_train))\nprint(\"Y label data : \", len(Y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\ntraining_set = datagen.flow(np.array(X_train),np.array(Y_train),batch_size=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"Load pretrained model:","metadata":{}},{"cell_type":"code","source":"weights='imagenet'\nconv_base = NASNetMobile(weights=weights,\n                     include_top=False,\n                    input_shape=(224,224,3))\nconv_base.trainable = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define Model","metadata":{}},{"cell_type":"code","source":"# Ne pas entraîner les 5 premières couches (les plus basses) \nfor layer in conv_base.layers[:5]:\n   layer.trainable = False\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(optimizer = Adam(lr=1e-4),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=['sparse_categorical_accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h5_path = \"model.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nhistory = model.fit_generator(\n    training_set,\n    validation_data=(np.array(X_val),np.array(Y_val)),\n    epochs=epochs, verbose=1,\n    callbacks=[checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30, 15))\nplt.plot(history.history[\"sparse_categorical_accuracy\"])\nplt.plot(history.history[\"val_sparse_categorical_accuracy\"])\nplt.title(\"Précisions des modèles\",fontsize=30)\nplt.yticks(size=20)\nplt.ylabel(\"Précisions\",fontsize=20)\nplt.xlabel(\"Epoch\",fontsize=20)\nplt.legend([\"Learning Accuracy\",\"Validation Accuracy\"],fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Test Data","metadata":{}},{"cell_type":"code","source":"images_pixels_pred = []\nfor row in samp_subm.itertuples():\n    image_id = row.id\n    file = image_id+'.jpg'\n    subpath = '/'.join([char for char in image_id[0:3]]) \n    finalpath = path+\"/test/\"+subpath+'/'+file\n\n    img = cv2.imread(finalpath)\n    img = cv2.resize(img, (img_size, img_size))\n    img = ImgToTreat(img)\n\n    images_pixels_pred.append(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.nasnet.preprocess_input)\nimage_to_predict = test_datagen.flow(np.array(images_pixels_pred),batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T22:54:11.982156Z","iopub.execute_input":"2021-09-30T22:54:11.982517Z"}}},{"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale=1./255)\nimage_to_predict = test_datagen.flow(np.array(images_pixels_pred),batch_size=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict_generator(image_to_predict, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(samp_subm.index)):\n    category = np.argmax(y_pred[i])\n    score = y_pred[i][np.argmax(y_pred[i])].round(2)\n    samp_subm.loc[i, 'landmarks'] = str(category)+' '+str(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_subm.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Export","metadata":{}},{"cell_type":"code","source":"samp_subm.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}